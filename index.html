<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <!-- Meta tags for search engines to crawl -->
      <meta name="google-site-verification" content="dyXyZEblSKIykynLd8NnNnSPO2-AxuLPvTsp_X7GDSg" />
      <!-- <meta name="robots" content="index,follow">  -->
      <meta name="keywords" content="Xubo Liu; 刘徐博; CVSSP; Centre for Vision, Speech and Signal Processing; University of Surrey; Beijing University of Posts and Telecommunications">
      <link rel="author" href="https://liuxubo717.github.io/">
      <title>Xubo Liu's Homepage</title>
      <style>
         @media screen and (max-device-width: 480px){
         body{
         -webkit-text-size-adjust: none;
         }
         }
         p { font-size : 16px; }
         h1 { font-size : 34px; margin : 0; padding : 0; }
         h2 { font-size : 20px; margin : 0; padding : 0; }
         h3 { font-size : 18px; margin : 8; padding : 0; }
         body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
         .title { width : 650px; margin : 20px auto; }
         .container { width : 880px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
         #bio {
         padding-top : 40px;
         }
         #me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
         #sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
         a { text-decoration : none; }
         a:hover { text-decoration : underline; }
         a, a:visited { color : #0050e7; }
         .publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
         .publication { clear : left; padding-bottom : 0px; }
         .publication p { height : 100px; padding-top : 5px;}
         .publication strong a { color : #0000A0; }
         .publication .links { position : relative; top : 15px }
         .publication .links a { margin-right : 20px; }
         .codelogo { margin-right : 10px; float : left; border : 0;}
         .code { clear : left; padding-bottom : 10px; vertical-align :middle;}
         .code .download a { display : block; margin : 0 15px; float : left;}
         .code strong a { color : #000; }
         .external a { margin : 0 10px; }
         .external a.first { margin : 0 10px 0 0; }
      </style>
      <script async="" src="./homepage_files/analytics.js"></script>
   </head>
   <body>
      <div class="title">
         <div id="sidebar"><img src="./homepage_files/xubo-white.jpg" vspace="50 px" width="200 px" id="me" itemprop="photo"></div>
         <div id="bio">
            <br>
            <h1>
               <span itemprop="name">
                  Xubo Liu <!-- <font size="5">刘徐博</font> -->
               </span>
            </h1>
            <br>
            <p style="line-height:23px;">
               PhD Candidate
               <br>
               <br>
               University of Surrey
               <br>
               <br>
               Office: 09BB01 (Building BB, 1st floor)
               <br>
               <br>
               Email: xubo.liu at surrey.ac.uk
            </p>
            <p class="external">
               <a href="https://scholar.google.com/citations?user=-OlNYSgAAAAJ&hl=zh-CN" class="first">Google Scholar</a>
               <a href="https://www.linkedin.com/in/xubo-liu-533b62191/">Linkedin</a>
            </p>
         </div>
      </div>
      <div class="container">
         <h2><font color="005AB5">Short Bio</font></h2>
         <p>
            Mr. Xubo Liu is a 3rd year Ph. D. student at the <a href="https://www.surrey.ac.uk/centre-vision-speech-signal-processing">Centre for Vision, Speech and Signal Processing (CVSSP)</a>, <a href="https://www.surrey.ac.uk/">University of Surrey</a> under the supervision of Prof. <a href="http://personal.ee.surrey.ac.uk/Personal/W.Wang/">Wenwu Wang</a> and Prof. <a href="https://www.surrey.ac.uk/people/mark-plumbley">Mark Plumbley</a>. His current research interests include machine listening and multimodal machine learning.
            <br>
            <br>
            Xubo received the <a href="./papers/Degree Certificate_QMUL.pdf">B. Eng. with First Class Honors</a> from <a href="https://www.qmul.ac.uk/">Queen Mary University of London (QMUL)</a> in July 2020. He was a research scientist intern at <a href="https://ai.facebook.com/">Meta AI</a> and a software engineer intern at <a href="https://www.research.ibm.com/labs/china/">IBM Research</a>.

            <br>
         </p>
      </div>
      <div class="container">
         <h2><font color="red">News!</font> </h2>
         <p>
            [2023-03] One <font color="red">CVPR</font> paper got accepted!
            <br>
            <br>
            [2023-03] One <font color="red">ICASSP</font> paper got accepted!
            <br>
            <br>
            [2022-11] One <font color="red">AAAI</font> paper got accepted!
            <br>
            <br>
            [2022-07] Join <font color="red">Meta AI Speech</font> as a research scientist intern!
            <br>
            <br>
            [2022-07] Achieved the <font color="red">2nd place</font> on the DCASE 2022 Challenge on Task 1: <a href="https://dcase.community/challenge2022/task-few-shot-bioacoustic-event-detection-results">"Few-shot Bioacoustic Event Detection"</a>.
            <br>
            <br>
            [2022-07] Achieved the <font color="red">2nd place</font> on the DCASE 2022 Challenge on Task 6B: <a href="https://dcase.community/challenge2022/task-language-based-audio-retrieval-results">"Language-Based Audio Retrieval"</a>.
            <br>
            <br>
            [2022-07] Achieved the <font color="red">3rd place</font> on the DCASE 2022 Challenge on Task 6A: <a href="https://dcase.community/challenge2022/task-automatic-audio-captioning-results">"Automated Audio Captioning"</a>.
            <br>
            <br>
            [2022-06] One <font color="red">MLSP 2022</font> papers got accepted!
            <br>
            <br>
            [2022-06] Five <font color="red">INTERSPEECH 2022</font> papers got accepted!
            <br>
            <br>
            [2022-05] Three <font color="red">EUSIPCO 2022</font> papers got accepted!
            <br>
            <br>
            [2022-01] Two <font color="red">ICASSP 2022</font> papers got accepted!
            <br>
            <br>
            [2021-09] Three <font color="red">DCASE Workshop 2021</font> papers got accepted!
            <br>
            <br>
            [2021-08] One <font color="red">MLSP 2021</font> paper got accepted!
            <br>
            <br>
            [2021-06] Achieved the <font color="red">3rd place</font> on the DCASE 2021 Challenge on Task 6: <a href="http://dcase.community/challenge2021/task-automatic-audio-captioning-results">"Automated Audio Captioning"</a>.
            <br>
            <br>
            [2021-06] One <font color="red">INTERSPEECH 2021</font> paper got accepted!
            <br>
            <br>
            [2020-09] Joined the <a href="https://www.surrey.ac.uk/centre-vision-speech-signal-processing/research/lab-machine-audition">Audio Research Group</a> at CVSSP as a Ph. D. student.
            <br>
            <br>
            [2020-07] Awarded the B. Eng. with First Class Honors, Queen Mary University of London.
            <br>
            <br>
            [2020-03] Awarded the Tuition Fees Waiver Scholarship, University of Surrey.
            <br>
            <br>
         </p>
      </div>
      <div class="container">
         <h2><font color="005AB5">Education</font></h2>
         <br>
         <div class="publication">
            <img src="./homepage_files/surrey.jpg" class="publogo" width="100 px">
            <p>
               <strong>
               University of Surrey
               </strong>
               <br>
               <br>
               Ph. D. in Electronic Engineering
               <br>
               <br>
               2020 - Present
               <br>
               <br>
            </p>
            <br>
         </div>
         <div class="publication">
            <img src="./homepage_files/qmul.jpg" class="publogo" width="100 px">
            <p>
               <strong>
               Queen Mary University of London (QMUL)
               </strong>
               <br>
               <br>
               <a href="./papers/Degree Certificate_QMUL.pdf">B. Eng. in Telecommunications Engineering</a>
               <br>
               <br>
               First Class Honors, 2016 - 2020
               <br>
               <br>
               <!--                 <span class="links">
                  <a href="https://arxiv.org/abs/2007.03777">PDF</a>
                  <a href="https://hahehi.github.io/placepedia.html">Dataset</a>
                  </span> -->
            </p>
            <br>
         </div>
      </div>
      <div class="container">
         <h2><font color="005AB5">Publications</font></h2>
         <br>
         <h3>2023</h3>
         <div class="publication">
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2210.00943.pdf">
               Simple Pooling Front-ends for Efficient Audio Classification
               </a>
               </strong>
               <br>
               <br>
               <b>Xubo Liu</b>, Haohe Liu, Qiuqiang Kong, Xinhao Mei, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/abs/2210.15088">
               Personalized Dialogue Generation with Persona-Adaptive Attention
               </a>
               </strong>
               <br>
               <br>
               Qiushi Huang, Yu Zhang, Tom Ko, <b>Xubo Liu</b>, Bo Wu, Wenwu Wang, Lilian H. Tang
               <br>
               <br>
               The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI), 2023
            </p>

         </div>
         <h3>2022</h3>
         <div class="publication">
            <p>
               <strong>
               <a href="https://arxiv.org/abs/2203.15147">
               Fish Feeding Intensity Assessment in Aquaculture: A New Audio Dataset AFFIA3K and A Deep Learning Algorithm
               </a>
               </strong>
               <br>
               <br>
                 Meng Cui*, <b>Xubo Liu*</b>, Tao Chen, Mark D. Plumbley, Daoliang Li, Wenwu Wang, et al.
               <br>
               <br>
               IEEE 32nd International Worlshop on Machine Learning for Signal Processing (MLSP), 2022
            </p>
             <p>
               <strong>
               <a href="https://arxiv.org/abs/2203.15147">
               Separate What You Describe: Language-Queried Audio Source Separation
               </a>
               </strong>
               <br>
               <br>
                 <b>Xubo Liu</b>, Haohe Liu, Qiuqiang Kong, Xinhao Mei, Mark D. Plumbley, Wenwu Wang, et al.
               <br>
               <br>
               INTERSPEECH 2022
            </p>
             <p>
               <strong>
               <a href="https://arxiv.org/abs/2203.15537">
               On Metric Learning for Audio-Text Cross-Modal Retrieval
               </a>
               </strong>
               <br>
               <br>
               Xinhao Mei, <b>Xubo Liu</b>, Jianyuan Sun, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               INTERSPEECH 2022
            </p>
             <p>
               <strong>
               <a href="">
               VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration
               </a>
               </strong>
               <br>
               <br>
               Haohe Liu*, <b>Xubo Liu</b>*, Qiuqiang Kong, Qian Tian, Yan Zhao, Deliang Wang, et al.
               <br>
               <br>
               INTERSPEECH 2022
            </p>
             <p>
               <strong>
               <a href="https://arxiv.org/abs/2203.14941">
               Neural Vocoder is All You Need for Speech Super-resolution
               </a>
               </strong>
               <br>
               <br>
               Haohe Liu, Woosung Choi, <b>Xubo Liu</b>, Qiuqiang Kong, Qiao Tian, DeLiang Wang
               <br>
               <br>
               INTERSPEECH 2022
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2203.02838.pdf">
               Leveraging Pre-trained BERT for Audio Captioning
               </a>
               </strong>
               <br>
               <br>
               <b>Xubo Liu</b>, Xinhao Mei, Qiushi Huang, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang, et al.
               <br>
               <br>
               The 30th European Signal Processing Conference (EUSIPCO 2022)
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2110.06691.pdf">
               Deep Neural Decision Forest for Acoustic Scene Classification
               </a>
               </strong>
               <br>
               <br>
               Jianyuan Sun*, <b>Xubo Liu</b>*, Xinhao Mei, Jinzheng Zhao, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang
               <br>
               <br>
               The 30th European Signal Processing Conference (EUSIPCO 2022)
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2110.06691.pdf">
               Diverse Audio Captioning via Adversarial Training
               </a>
               </strong>
               <br>
               <br>
               Xinhao Mei, <b>Xubo Liu</b>, Jianyuan Sun, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022
            </p>
            <p>
               <strong>
               <a href="https://eprints.whiterose.ac.uk/183718/">
               Audio-Visual Tracking of Multiple Speakers via a PMBM Filter
               </a>
               </strong>
               <br>
               <br>
               Jinzheng Zhao, Peipei Wu, <b>Xubo Liu</b>, Yong Xu, Lyudmila Mihaylova, Simon Godsill, Wenwu Wang
               <br>
               <br>
               IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022
            </p>
         </div>
         <h3>2021</h3>
         <div class="publication">
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2107.09998.pdf">
               Conditional Sound Generation Using Neural Discrete Time-Frequency Representation Learning
               </a>
               </strong>
               <br>
               <br>
               <b>Xubo Liu</b>, Turab Iqbal, Jinzheng Zhao, Qiushi Huang, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               IEEE 31st International Worlshop on Machine Learning for Signal Processing (MLSP), 2021
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2107.09990.pdf">
               CL4AC: A Contrastive Loss for Audio Captioning
               </a>
               </strong>
               <br>
               <br>
               <b>Xubo Liu</b>*, Qiushi Huang*, Xinhao Mei, Tom Ko, H Lilian Tang, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               The 6th International Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2021)
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2107.09817.pdf">
               Audio Captioning Transformer
               </a>
               </strong>
               <br>
               <br>
               Xinhao Mei, <b>Xubo Liu</b>, Qiushi Huang, Mark D. Plumbley, Wenwu Wang
               <br>
               <br>
               The 6th International Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2021)
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2108.02752.pdf">
               An Encoder-Decoder Based Audio Captioning System with Transfer and Reinforcement Learning
               </a>
               </strong>
               <br>
               <br>
               Xinhao Mei, Qiushi Huang, <b>Xubo Liu</b>, Shengchen Li, Tom Ko, Mark D. Plumbley, Wenwu Wang, et al.
               <br>
               <br>
               The 6th International Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2021)
            </p>
            <p>
               <strong>
               <a href="https://arxiv.org/pdf/2107.09099.pdf">
               Token-Level Supervised Contrastive Learning for Punctuation Restoration
               </a>
               </strong>
               <br>
               <br>
               Qiushi Huang, Tom Ko, H Lilian Tang, <b>Xubo Liu</b>, Bo Wu
               <br>
               <br>
               INTERSPEECH 2021
               <!--                 <span class="links">
                  <a href="https://arxiv.org/abs/2007.03777">PDF</a>
                  <a href="https://hahehi.github.io/placepedia.html">Dataset</a>
                  </span> -->
            </p>
         </div>
      </div>
      <div class="container">
         <h2><font color="005AB5">Teaching</font></h2>
         <p>
            <li>COM3025 - Deep Learning and Advanced AI (Dr. Lilian Tang)</li>
            <br>
            <li>COM2028 - Artificial Intelligence (Dr. Lilian Tang)</li>
         </p>
      </div>
      <div class="container">
         <h2><font color="005AB5">Awards & Honors</font></h2>
         <p>
            <li>Second place on the DCASE 2022 Challenge Task 5: “Few Shot Bioacoustic Event Detection”</li>
            <br>
            <li>Second place on the DCASE 2022 Challenge Task 6A: “Language-based Audio Retrieval"</li>
            <br>
            <li>Third place on the DCASE 2022 Challenge Task 6B: “Automated Audio Captioning"</li>
            <br>
            <li>Third place on the DCASE 2021 Challenge Task 6: “Automated Audio Captioning"</li>
            <br>
            <li>Tuition Fees Waiver PhD Scholarship, University of Surrey</li>
         </p>
      </div>
      <div class="container">
         <h2><font color="005AB5">Professional Services</font></h2>
         <p>
            <li>Serving as a reviewer for ICASSP 2023, Interspeech 2023</li>
            <br>
            <li>Serving as a reviewer for IEEE/ACM Transactions on Audio, Speech and Language Processing</li>
            <br>
         <li>Special session chair of <a href="https://eusipco2023.org/special_sessions.html">”Multimodal Learning for Audio and Language”</a> at EUSIPCO 2023</li>
         </p>
      </div>

   </body>
</html>