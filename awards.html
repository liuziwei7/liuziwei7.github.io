<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Ziwei Liu; 刘子纬; Computer Vision; Deep Learning; Computer Graphics; Multimedia Lab; MMLAB; The Chinese University of Hong Kong; CUHK; UC Berkeley; ICSI; Nanyang Technological University; NTU">
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.min.js"></script>
<link rel="author" href="https://liuziwei7.github.io/">

    <title>Ziwei Liu - Awards</title>
    <style>

@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : rgb(224, 224, 224); }
.title { width : 650px; margin : 20px auto; }
.container { width : 750px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
.container_title { width : 750px; margin : 20px auto; border-radius: 10px;  padding : 20px;  clear:both;}
.iframe_video {float: left; margin-right: 30px}
#bio {
    padding-top : 20px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; margin-right : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
    </style>
    <script async="" src="./homepage_files/analytics.js"></script>
</head>

<body>

    <div class="container">
        <h2>
            <a href="./index.html">Home</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./research.html">Research</a>&nbsp;&nbsp;&middot;&nbsp;
            <a href="./publications.html">Publications</a>&nbsp;&nbsp;&middot;&nbsp;
            <a href="./softwares.html">Softwares</a>&nbsp;&nbsp;&middot;&nbsp;
            <a href="./team.html">Team</a>&nbsp;&nbsp;&middot;&nbsp;
            <a href="./services.html">Services</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./awards.html">Awards</a>
        </h2>
    </div>

    <div class="container">
    	<h2>Awards</h2>
    	<div>
    		<p>
    		<br>
            2025: <em><a href="https://eyeline-labs.github.io/VChain/">VChain</a> is selected as <a href="https://knowledgemr-workshop.github.io/">ICCV 2025 KnowledgeMR Workshop Outstanding Paper Award</a></em>
            <br>
            <br>
            2025: <em><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> is awarded CCF-CV Test of Time Award</a></em>   
            <br>
            <br>
            2025: <em><a href="https://liuziwei7.github.io/projects/DGCNN">DGCNN</a> is selected as <a href="https://dl.acm.org/journal/tog">The Most Cited Paper in TOG History</a></em>
            <br>
            <br>
            2025: <em><a href="https://vchitect.github.io/Evaluation-Agent-project/">Evaluation Agent</a> is selected as <a href="https://2025.aclweb.org/program/awards/">ACL 2025 SAC Highlights Award</a></em>         
            <br>
            <br>
            2025: <em><a href="https://github.com/KaiyangZhou/CoOp">CoOp</a> is selected as <a href="https://www.waicyop.cn/article/15">WAIC 2025 Youth Outstanding Paper Award</a></em>         
            <br>
            <br>
            2024: <em><a href="https://research.google/programs-and-events/featured-research-collaborations/south-asia-southeast-asia-awards-recipients/?filtertab=2024">Google Academic Research Award (South Asia & Southeast Asia)</a></em>         
            <br>
            <br>
            2024: <em><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> is awarded <a href="https://tc.computer.org/tcpami/awards/pami-mark-everingham-prize/">PAMI Mark Everingham Prize</a></em>         
            <br>
            <br>
            2024: <em><a href="https://www.aysfellowship.org/math-cs-fellows/ziweiliu-2024">Asian Young Scientist Fellowship</a></em>         
            <br>
            <br>  
            2024: <em><a href="https://www.nature.com/articles/s42256-021-00393-0">Wildlife Identification</a> is selected as <a href="https://sheitc.sh.gov.cn/zxxx/20240618/23f50b2ba3d0422daf775915f43dfa61.html">WAIC 2024 Youth Outstanding Paper Award</a></em>
            <br>
            <br>
            2024: <em><a href="https://liuziwei7.github.io/projects/DGCNN">DGCNN</a> is selected as <a href="https://dl.acm.org/journal/tog">Second Most Cited Paper in TOG History</a></em>
            <br>
            <br>
            2024: <em>Winner of the <a href="https://waymo.com/open/challenges/">Waymo Open Dataset Challenge</a></em>
            <br>
            <br>
            2023: <em><a href="https://www.innovatorsunder35.com/the-list/ziwei-liu/">MIT Technology Review Innovators under 35 Asia Pacific</a></em>         
            <br>
            <br>  
            2023: <em><a href="https://ldkong.com/LaserMix">LaserMix</a> is selected as <a href="http://www.premiasg.org/for-members/premia-best-student-paper-awards/">PREMIA Best Student Paper Award</a></em>
            <br>
            <br>
            2023: <em><a href="https://liuziwei7.github.io/projects/DGCNN">DGCNN</a> is selected as <a href="https://www.icbs.cn/en/web/index/18009_1581229__">ICBS 2023 Frontiers of Science Award</a></em>
            <br>
            <br>
            2023: <em><a href="https://omniobject3d.github.io/">OmniObject3D</a> is selected as <a href="https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers">CVPR 2023 Best Paper Award Candidate</a></em>
            <br>
            <br>
            2023: <em><a href="https://ldkong.com/Robo3D">Robo3D</a> is selected as <a href="https://opendrivelab.com/sr4ad/iclr23">ICLR 2023 SR4AD Workshop Best Paper Award</a></em>
            <br>
            <br>
            2022: <em><a href="https://github.com/KaiyangZhou/CoOp">CoCoOp</a> is selected as <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">Top-100 Most Cited AI Paper in 2022</a></em>
            <br>
            <br>
            2022: <em><a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw">World’s Top 2% Scientists</a></em>         
            <br>
            <br>
            2022: <em>Winner of the <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/">Computer Vision in the Wild Challenge</a></em>         
            <br>
            <br>
            2022: <em><a href="https://www.jiqizhixin.com/articles/2022-09-07-6">WAIC Yunfan Award</a></em>         
            <br>
            <br>
            2022: <em><a href="https://xueshu.baidu.com/usercenter/index/aischolar2022">Baidu AI Young Scholars in Computer Vision</a></em>         
            <br>
            <br>
            2021: <em><a href="https://liuziwei7.github.io/projects/DGCNN">DGCNN</a> is selected as <a href="https://scholar.google.com/citations?&view_op=list_hcore&venue=eq0ghEO5jswJ.2021">Most Cited Paper within 5 Years in TOG</a></em>
            <br>
            <br>
            2021: <em><a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html">DeepFashion</a> is selected as <a href="https://scholar.google.com/citations?view_op=list_hcore&venue=FXe-a9w0eycJ.2021&cstart=80">Top-100 Most Cited Paper within 5 Years in CVPR</a></em>
            <br>
            <br>
			2021: <em><a href="https://www.aminer.org/ai2000/cv">AI 2000 Most Influential Scholars in Computer Vision</a></em>         
			<br>
            <br>
            2021: <em><a href="https://www.nvidia.com/en-sg/industries/higher-education-research/academic-grant-program/">Nvidia Academic Grant Program</a></em>         
            <br>
            <br>  
            2020: <em><a href="https://www.ntu.edu.sg/engineering/aboutus/ourpeople/research-talent-development/nanyang-assistant-professorship-(nap)">Nanyang Assistant Professorship</a></em>
            <br>
            <br>
			2020: <em><a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> is selected as <a href="https://scholar.google.com/citations?&view_op=list_hcore&venue=uDnJSYNMB80J.2020">Top-10 Most Cited Paper Within 5 Years in ICCV</a></em>
			<br>
            <br>
			2020: <em>Winner of the Video Virtual Try-on Challenge</em>
			<br>
            <br>
 			2019: <em><a href="https://liuziwei7.github.io/projects/LongTail.html">OLTR</a> is selected as HKSTP Best Paper Award</em>
 			<br>
            <br>
			2019: <em>Winner of the FAIR Self-Supervision Challenge</em>
			<br>
            <br>
 			2018: <em>Winner of the COCO Detection Challenge</em>
 			<br>
            <br>
 			2017: <em>Winner of the DAVIS Video Segmentation Challenge</em>
 			<br>
            <br>
			2017: <em>ICCV Young Researcher Award</em>
   			</p>
    	</div>
    </div>

    <div class="container">
        <h2>Talks <small><a href="https://www.youtube.com/channel/UCRg_2TVVQc7PgFlXzKXhbTg">[Youtube]</a></small> </h2>
        <div>
            <p>
            <br>
            2025 @ ACM MM: <em><a href="./papers/worldmodel_slides.pdf">From Multimodal Generative Models to Dynamic World Modeling</a></em>
            <br>
            <br>
            2025 @ ICCV: <em><a href="./papers/egointelligence_slides.pdf">From Egocentric Perception to Embodied Intelligence: Building the World in First Person</a></em>
            <br>
            <br>
            2025 @ ICCV: <em><a href="./papers/generativereasoning_slides.pdf">Multimodal Reasoning for Human-Centric Generative Models</a></em>
            <br>
            <br>
            2025 @ ICCV: <em><a href="./papers/nativelmm_slides.pdf">Native Multimodal Models: Architecture, Post-Training, and Evaluation</a></em>
            <br>
            <br>
            2025 @ Mini3DV: <em><a href="./papers/predictiveworldmodel_slides.pdf">Predictive World Models: Faithfulness, Interactiveness and Planning</a></em>
            <br>
            <br>
            2025 @ CVPR: <em><a href="./papers/autocharacter_slides.pdf">The Path from Marionette to Autonomous 3D Characters</a></em>
            <br>
            <br>
            2025 @ CVPR: <em><a href="./papers/worldmodel_slides.pdf">From Multimodal Generative Models to Dynamic World Modeling</a></em>
            <br>
            <br>
            2025 @ VALSE: <em><a href="./papers/worldmodel_slides.pdf">From Multimodal Generative Models to Dynamic World Modeling</a></em>
            <br>
            <br>
            2025 @ ICLR: <em><a href="./papers/worldmodel_slides.pdf">From Multimodal Generative Models to Dynamic World Modeling</a></em>
            <br>
            <br>
            2025 @ China3DV: <em><a href="./papers/worldmodel_slides.pdf">From Multimodal Generative Models to Dynamic World Modeling</a></em>
            <br>
            <br>
            2024 @ NeurIPS: <em><a href="./papers/3dsimulator_slides.pdf">From High-fidelity 3D Generative Models to Dynamic Embodied Learning</a></em>
            <br>
            <br>
            2024 @ ACM MM: <em><a href="./papers/genai2024_slides.pdf">Multi-Modal Generative AI with Foundation Models</a></em>
            <br>
            <br>
            2024 @ ECCV: <em><a href="./papers/3dsimulator_slides.pdf">From High-fidelity 3D Generative Models to Dynamic Embodied Learning</a></em>
            <br>
            <br>
            2024 @ CVPR: <em><a href="./papers/genai2024_slides.pdf">Multi-Modal Generative AI with Foundation Models</a></em>
            <br>
            <br>
            2024 @ CVPR: <em><a href="./papers/vchitect_slides.pdf">Vchitect: Efficient and Scalable Video Generation</a></em>
            <br>
            <br>
            2024 @ CVPR: <em><a href="./papers/3dtopia_slides.pdf">3DTopia: Foundation Ecosystem for 3D Generative Models</a></em>
            <br>
            <br>
            2024 @ CVPR: <em><a href="./papers/aiassistant_slides.pdf">Building Open-World Multi-Modal AI Assistant</a></em>
            <br>
            <br>
            2024 @ CVPR: <em><a href="./papers/lmmslab_slides.pdf">LMMs-Lab: Building Multimodal Intelligence</a></em>
            <br>
            <br>
            2024 @ WWW: <em><a href="./papers/aiassistant_slides.pdf">Building Open-World Multi-Modal AI Assistant</a></em>
            <br>
            <br>
            2024 @ VALSE: <em><a href="./papers/vchitect_slides.pdf">Vchitect: Building Open Source Foundation System for Video Generation</a></em>
            <br>
            <br>
            2024 @ WACV: <em><a href="./papers/genai_slides.pdf">Multi-Modal Generative AI with Foundation Models</a></em>
            <br>
            <br>
            2023 @ ACM MM: <em><a href="./papers/genai_slides.pdf">Multi-Modal Generative AI with Foundation Models</a></em>
            <br>
            <br>
            2023 @ IJCAI: <em><a href="./papers/aiassistant_slides.pdf">Towards Building Practical AI Assistant</a></em>
            <br>
            <br>
            2023 @ USTC: <em><a href="./papers/multimodalai_slides.pdf">Multi-Modal Generative AI with Foundation Models</a></em>
            <br>
            <br>
            2023 @ CVPR: <em><a href="./papers/prompting_slides.pdf">Prompting in Visual Generation</a></em>
            <br>
            <br>
            2023 @ CVPR: <em><a href="./papers/aiassistant_slides.pdf">Towards Building Practical AI Assistant</a></em>
            <br>
            <br>
            2023 @ China3DV: <em><a href="./papers/visualaigc_slides.pdf">Visual AIGC with Foundation Models</a></em>
            <br>
            <br>
            2023 @ SCA: <em><a href="./papers/aigc_slides.pdf">AI-Driven Visual Content Generation</a></em>
            <br>
            <br>
            2022 @ PRCV: <em><a href="./papers/robust3d_slides.pdf">Robust and Data-Efficient 3D Perception</a></em>
            <br>
            <br>
            2022 @ ACCV: <em><a href="./papers/visualediting_slides.pdf">Human-Centric Visual Generation and Editing</a></em>
            <br>
            <br>
            2022 @ ICLR: <em><a href="./papers/rethinkinggeneralization_slides.pdf">Rethinking Generalization in Vision Models: Architectures, Modalities, and Beyond</a></em>
            <br>
            <br>
            2021 @ M2VIP: <em><a href="./papers/3dperception_slides.pdf">3D Perception from Partial Observations</a></em>
            <br>
            <br>
            2021 @ AI Technology Summer School: <em><a href="./papers/aimedia_slides.pdf">AI-Synthesized Media and How to Detect Them</a></em>
            <br>
            <br>
            2020 @ ACM MM: <em><a href="./papers/openworldhuman_slides.pdf">Sensing, Understanding and Synthesizing Humans in an Open World</a></em>
            <br>
            <br>
            2019 @ ICCV: <em><a href="./papers/diversehuman_slides.pdf">Learning Diverse Human Representation in the Wild</a></em>
            <br>
            <br>
            2019 @ VALSE: <em><a href="./papers/glimpseofdetectron_slides.pdf">The Glimpse of Detectron: Dynamic Forwarding and Routing in Modern Detectors</a></em>
            <br>
            <br>
            2018 @ Berkeley: <em><a href="./papers/videoparsing_slides.pdf">Learning Video Parsing, Tracking and Synthesis in the Wild</a></em>
            <br>
            <br>
            2017 @ VALSE: <em><a href="./papers/humancentric_slides.pdf">Deep Learning Human-centric Representation in the Wild</a></em>
            <br>
            <br>
            <!-- <iframe width="320" height="200" src="https://www.youtube.com/embed/kOLLnJgFFkk" frameborder="0" allowfullscreen></iframe>
            <br>
            <br> -->
            2016 @ Google: <em><a href="./papers/deepfashion_slides.pdf">Deep Fashion Understanding</a></em>
            <br>
            <br>
            2015 @ CUHK: <em><a href="./papers/visualstructures_slides.pdf">Formulating Structure for Vision Problems</a></em>
            <br>
            <br>
            2014 @ SIGGRAPH Asia: <em><a href="./papers/burstdenoising_slides.pdf">Fast Burst Images Denoising</a></em>
            <br>
            <br>
            2013 @ HUST: <em><a href="./papers/microscopicimageseg_slides.pdf">Pathological Microscopic Image Segmentation (in Chinese)</a></em>
            </p>
        </div>
    </div>

    <div class="container">
        <h2>Misc</h2>
    	<div>
    		<p>
    		<br>
    		<span class="links">
                <a href="https://blindsights.blogspot.com/">Blog</a>
                &nbsp;&nbsp;&nbsp;
                <a href="https://goo.gl/photos/CE8chhuJwTAcZqtL7">Doodles</a>
                &nbsp;&nbsp;&nbsp;
                <a href="https://www.youtube.com/playlist?list=PLua4XbbBXzFciVTrYVhZB3Kt7-ZERzzXp">Microcinema</a>
            </span>
   			</p>
    	</div>
    </div>

</body></html>
