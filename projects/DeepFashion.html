<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0059)http://handtracker.mpi-inf.mpg.de/projects/FastHandTracker/ -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets
are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.>
<meta name="keywords" content="DeepFashion; fashion dataset; clothes recognition; clothes retrieval; FashionNet; deep learning;">
<link rel="author" href="personal.ie.cuhk.edu.hk/~lz013/">

<!-- Fonts and stuff -->
<link href="./deepfashion/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./deepfashion/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./deepfashion/iconize.css">
<script async="" src="./deepfashion/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</h1>

	<div class="authors">
	  <a href="http://personal.ie.cuhk.edu.hk/~lz013/">Ziwei Liu</a>&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~pluo/">Ping Luo</a>&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="">Shi Qiu</a>&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a>
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory, </a>
	  <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>
	</div>

	<div class="venue">IEEE Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2016.thecvf.com/" target="_blank">CVPR</a>) 2016</div>
      </div>

      
      <center><img src="./deepfashion/intro.png" border="0" width="100%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets
are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.
	</p>
      </div>
      
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
            <li class="grid">
	      <div class="griditem">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" target="_blank" class="imageLink"><img src="./deepfashion/paper.png"></a><br>
		  <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" target="_blank">Paper</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="../papers/deepfashion_supp.pdf" target="_blank" class="imageLink"><img src="./deepfashion/supp.png"></a><br>
		  <a href="../papers/deepfashion_supp.pdf" target="_blank">Supplementary</a>
		</div>
	      </li>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="../papers/deepfashion_poster.pdf" target="_blank" class="imageLink"><img src="./deepfashion/poster.jpg"></a><br>
		  <a href="../papers/deepfashion_poster.pdf" target="_blank">Poster</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/liuziwei7/fashion-detection" target="_blank" class="imageLink"><img src="./deepfashion/code.png"></a><br>
		  <a href="https://github.com/liuziwei7/fashion-detection" target="_blank">Fashion Detection</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section data">
	<h2>Dataset</h2>
		
		<p>We contribute <b>DeepFashion</b> database, a large-scale clothes database, which has several appealing properties:</p>

		<ul>
		<li><p>First, DeepFashion contains over <b>800,000</b> diverse fashion images ranging from well-posed shop images to unconstrained consumer photos.<!--, making it twice the size of the previous largest clothing dataset.--></p>
		</li>
		<li><p>Second, DeepFashion is annotated with rich information of clothing items. Each image in this dataset is labeled with <b>50</b> categories, <b>1,000</b> descriptive attributes, bounding box and clothing landmarks.</p>
		</li>
		<li><p>Third, DeepFashion contains over <b>300,000</b> cross-pose/cross-domain image pairs.</p>
		</li>
		</ul>
		<b>Four benchmarks</b> are developed using the DeepFashion database, including <b>Attribute Prediction</b>, <b>Consumer-to-shop Clothes Retrieval</b>, <b>In-shop Clothes Retrieval</b>, and <b>Landmark Detection</b>. The data and annotations of these benchmarks can be also employed as the training and test sets for the following computer vision tasks, such as Clothes Detection, Clothes Recognition, and Image Retrieval. 
		<br>
		<br>
		<br>
		<center>
      	<a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank" class="imageLink"><img src="./deepfashion/dataset.jpg" border="2" width="70%"></a><br>
      	<a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank">DeepFashion Dataset</a>
      </div></center>

<div class="section demo">
	<h2>Demo</h2><center>
		<br>
      	<a href="http://fashion.sensetime.com/" target="_blank" class="imageLink"><img src="./deepfashion/demo.png" border="2" width="70%"></a><br>
      	<a href="http://fashion.sensetime.com/" target="_blank">FashionEye Search Engine</a>
      </div></center>

<br>

<div class="section press">
	<h2>Press Coverage</h2>
      
      <br>

      <a href="http://mt.sohu.com/20160701/n457325594.shtml">Sohu Technology News (in Chinese)</a>
      </div>



<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{liu2016deepfashion,
 author = {Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang},
 title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 month = June,
 year = {2016} 
}</pre>
	  </div>
      </div>

</body></html>