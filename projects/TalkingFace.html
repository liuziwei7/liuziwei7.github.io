<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Talking face generation aims to synthesize a sequence of face images that correspond to given speech semantics. However, when people talk, the subtle movements of their face region are usually a complex combination of the intrinsic face appearance of the subject and also the extrinsic speech to be delivered. Existing works either focus on the former, which constructs the specific face appearance model on a single subject; or the latter, which models the identity-agnostic transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We assume the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. The disentangled representation has an additional advantage that both audio and video can serve as the source of speech information for generation. Extensive experiments show that our proposed approach can generate realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns. We also demonstrate the learned audio-visual representation is extremely useful for applications like automatic lip reading and audio-video retrieval.">
<meta name="keywords" content="talking face generation; GAN; audio-visual representation; lipreading; deep learning;">
<link rel="author" href="https://liuziwei7.github.io/">

<!-- Fonts and stuff -->
<link href="./talkingface/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./talkingface/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./talkingface/iconize.css">
<script async="" src="./talkingface/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</h1>

	<div class="authors">
	  <a href="">Hang Zhou</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://liuyu.us/">Yu Liu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~pluo/">Ping Luo</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory, </a>
	  <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>
	</div>

	<div class="venue">AAAI Conference on Artificial Intelligence (<a href="http://www.aaai.org/Conferences/AAAI/aaai16.php" target="_blank">AAAI</a>) 2019 </div>
      </div>
      
      <center><img src="./talkingface/intro.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Talking face generation aims to synthesize a sequence of face images that correspond to given speech semantics. However, when people talk, the subtle movements of their face region are usually a complex combination of the intrinsic face appearance of the subject and also the extrinsic speech to be delivered. Existing works either focus on the former, which constructs the specific face appearance model on a single subject; or the latter, which models the identity-agnostic transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We assume the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. The disentangled representation has an additional advantage that both audio and video can serve as the source of speech information for generation. Extensive experiments show that our proposed approach can generate realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns. We also demonstrate the learned audio-visual representation is extremely useful for applications like automatic lip reading and audio-video retrieval.
	</p>
      </div>

<div class="section demo">
	<h2>Demo Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/-J2zANwdjcQ" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>
      
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1807.07860" target="_blank" class="imageLink"><img src="./talkingface/paper.jpg"></a><br>
		  <a href="https://arxiv.org/abs/1807.07860" target="_blank">Paper</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS" target="_blank" class="imageLink"><img src="./talkingface/code.png"></a><br>
		  <a href="https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS" target="_blank">Code and Models</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{zhou2018talking,
  title={Talking Face Generation by Adversarially Disentangled Audio-Visual Representation},
  author={Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2019}
}</pre>
	  </div>
      </div>

</body></html>